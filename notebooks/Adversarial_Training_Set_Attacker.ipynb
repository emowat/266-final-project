{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXQXnbWGxnT8",
        "outputId": "48557769-ddb1-4c72-d742-8ce41765f696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q textattack\n",
        "!pip install -q transformers\n",
        "!pip install -q flash-attn\n",
        "!pip install -q datasets\n",
        "!pip install -q nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alnj37mvxqSE",
        "outputId": "0fab43ab-0f1e-4b96-e852-3daae6f87703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.7/445.7 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.1/203.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m146.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import nltk\n",
        "import math\n",
        "from textattack import attack_recipes\n",
        "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
        "from textattack.attack_results import SuccessfulAttackResult\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# --- Configuration ---\n",
        "SEED = 42\n",
        "\n",
        "# Prefix for file paths within the mounted Google Drive\n",
        "DRIVE_PREFIX = \"/content/drive/MyDrive/266-final-project-data\"\n",
        "\n",
        "# --- Model and Input ---\n",
        "MODEL_PATH = os.path.join(DRIVE_PREFIX, \"guardrail_model_DistilBERT\") # Use worst v1 model\n",
        "ORIGINAL_TRAIN_FILE = os.path.join(DRIVE_PREFIX, \"train_dataset.csv\")\n",
        "ORIGINAL_VAL_FILE = os.path.join(DRIVE_PREFIX, \"val_dataset.csv\")\n",
        "ORIGINAL_TEST_FILE = os.path.join(DRIVE_PREFIX, \"test_dataset.csv\")\n",
        "\n",
        "# --- Output Files ---\n",
        "# create outputs for BOTH attack types\n",
        "ATTACK_TRAIN_TF_OUTPUT_FILE = os.path.join(DRIVE_PREFIX, \"training_set_attack_textfooler.csv\")\n",
        "ATTACK_VAL_TF_OUTPUT_FILE = os.path.join(DRIVE_PREFIX, \"validation_set_attack_textfooler.csv\")\n",
        "ATTACK_TRAIN_DWB_OUTPUT_FILE = os.path.join(DRIVE_PREFIX, \"training_set_attack_deepwordbug.csv\")\n",
        "ATTACK_VAL_DWB_OUTPUT_FILE = os.path.join(DRIVE_PREFIX, \"validation_set_attack_deepwordbug.csv\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(SEED)\n",
        "print(f\"Using random seed: {SEED}\")\n",
        "\n",
        "def load_malicious_prompts_from_set(file_path: str) -> list:\n",
        "    \"\"\"\n",
        "    Loads a dataset (train or val) and filters for ALL 'Malicious' prompts.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Error: Dataset file not found at {file_path}. Skipping.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Loading data from {file_path}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "    # --- THIS IS THE CORRECTED LOGIC ---\n",
        "    # We attack ALL prompts that are labeled as Malicious\n",
        "    attack_prompts_df = df[df[\"Final_Label\"] == \"Malicious\"]\n",
        "    # ---\n",
        "\n",
        "    if \"Obfuscated_Prompt\" not in attack_prompts_df.columns:\n",
        "        print(\"Error: 'Obfuscated_Prompt' column not found.\")\n",
        "        return []\n",
        "\n",
        "    return attack_prompts_df['Obfuscated_Prompt'].dropna().astype(str).tolist()\n",
        "\n",
        "def save_prompts_to_csv(prompts: list, file_path: str):\n",
        "    \"\"\"Saves a list of prompts to a new CSV file.\"\"\"\n",
        "    print(f\"\\nSaving {len(prompts)} successful attack prompts to {file_path}...\")\n",
        "    try:\n",
        "        df = pd.DataFrame(prompts, columns=[\"Prompt\"])\n",
        "        df.to_csv(file_path, index=False)\n",
        "        print(f\"Successfully saved file: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving {file_path}: {e}\")\n",
        "\n",
        "def run_attack(prompts: list, attack, attack_name: str) -> list:\n",
        "    \"\"\"\n",
        "    Runs an attack on a list of prompts and returns the successful perturbations.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Attacking {len(prompts)} prompts with {attack_name} ---\")\n",
        "    successful_attacks = []\n",
        "\n",
        "    # We must label all these prompts as \"1\" (Malicious)\n",
        "    # The attack's goal is to flip this label to \"0\" (Benign)\n",
        "    ground_truth_label = 1\n",
        "\n",
        "    for prompt in tqdm(prompts, desc=f\"Attacking with {attack_name}\"):\n",
        "        try:\n",
        "            result = attack.attack(prompt, ground_truth_label)\n",
        "\n",
        "            # Check if the attack was successful\n",
        "            if isinstance(result, SuccessfulAttackResult):\n",
        "                successful_attacks.append(result.perturbed_text()) # Call the method\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Attack failed for prompt '{prompt[:50]}...'. Skipping. Error: {e}\")\n",
        "\n",
        "    print(f\"\\n{attack_name} successfully fooled the model on {len(successful_attacks)} out of {len(prompts)} prompts.\")\n",
        "    return successful_attacks\n",
        "\n",
        "def main():\n",
        "    print(\"Starting adversarial attack script on TRAINING and VALIDATION data...\")\n",
        "\n",
        "    # --- Download NLTK resources required by TextAttack ---\n",
        "    try:\n",
        "        print(\"Downloading NLTK resources (for textattack)...\")\n",
        "        nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "        nltk.download('omw-1.4', quiet=True)\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "        print(\"NLTK resources downloaded.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not download NLTK resources. Attacks may fail. Error: {e}\")\n",
        "    # ---\n",
        "\n",
        "    # --- 1. Check for GPU ---\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"\\n\\033[93mWARNING: No GPU detected. This will be VERY slow.\\033[0m\")\n",
        "        device = torch.device(\"cpu\")\n",
        "    else:\n",
        "        device = torch.device(\"cuda:0\")\n",
        "        print(f\"\\nRunning on GPU: {torch.cuda.get_device_name(0)}\\n\")\n",
        "\n",
        "    # --- 2. Load Your Trained Model & Tokenizer ---\n",
        "    print(f\"Loading fine-tuned model from {MODEL_PATH}...\")\n",
        "\n",
        "    clean_model_path = MODEL_PATH\n",
        "    if clean_model_path.startswith(\"./\"):\n",
        "        clean_model_path = clean_model_path[2:]\n",
        "\n",
        "    try:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(clean_model_path).to(device)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(clean_model_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model from '{clean_model_path}': {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 3. Wrap Model for TextAttack ---\n",
        "    print(\"Wrapping model for TextAttack...\")\n",
        "    model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
        "\n",
        "    # --- 4. Load Raw Prompts from TRAIN and VAL sets ---\n",
        "    train_prompts_to_attack = load_malicious_prompts_from_set(ORIGINAL_TRAIN_FILE)\n",
        "\n",
        "    val_prompts_to_attack = load_malicious_prompts_from_set(ORIGINAL_VAL_FILE)\n",
        "    test_prompts_to_attack = load_malicious_prompts_from_set(ORIGINAL_TEST_FILE)\n",
        "    combined_val_test_prompts_to_attack = val_prompts_to_attack + test_prompts_to_attack\n",
        "\n",
        "    if not train_prompts_to_attack or not combined_val_test_prompts_to_attack:\n",
        "        print(\"No prompts to attack. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loaded {len(train_prompts_to_attack)} malicious prompts from the training set.\")\n",
        "    print(f\"Loaded {len(combined_val_test_prompts_to_attack)} malicious prompts from the combined val/test sets.\")\n",
        "\n",
        "    # --- 4b. Shuffle and split the prompt lists ---\n",
        "    print(\"Shuffling and splitting prompt lists for 50/50 attacks...\")\n",
        "    random.shuffle(train_prompts_to_attack)\n",
        "    random.shuffle(combined_val_test_prompts_to_attack)\n",
        "\n",
        "    train_split_index = math.ceil(len(train_prompts_to_attack) / 2)\n",
        "    val_split_index = math.ceil(len(combined_val_test_prompts_to_attack) / 2)\n",
        "\n",
        "    train_tf_prompts = train_prompts_to_attack[:train_split_index]\n",
        "    train_dwb_prompts = train_prompts_to_attack[train_split_index:]\n",
        "\n",
        "    val_tf_prompts = combined_val_test_prompts_to_attack[:val_split_index]\n",
        "    val_dwb_prompts = combined_val_test_prompts_to_attack[val_split_index:]\n",
        "\n",
        "\n",
        "    # --- 5. Initialize Attacks ---\n",
        "    print(\"\\nInitializing TextFooler attack...\")\n",
        "    textfooler_attack = attack_recipes.TextFoolerJin2019.build(model_wrapper)\n",
        "\n",
        "    print(\"\\nInitializing DeepWordBug attack...\")\n",
        "    deepwordbug_attack = attack_recipes.DeepWordBugGao2018.build(model_wrapper)\n",
        "\n",
        "\n",
        "    # --- 6. Run Attack on TRAINING data (50/50 split) ---\n",
        "    train_tf_successes = run_attack(\n",
        "        train_tf_prompts, # <-- Only first 50%\n",
        "        textfooler_attack,\n",
        "        \"TextFooler (Train Set)\"\n",
        "    )\n",
        "    save_prompts_to_csv(train_tf_successes, ATTACK_TRAIN_TF_OUTPUT_FILE)\n",
        "\n",
        "    train_dwb_successes = run_attack(\n",
        "        train_dwb_prompts, # <-- Only second 50%\n",
        "        deepwordbug_attack,\n",
        "        \"DeepWordBug (Train Set)\"\n",
        "    )\n",
        "    save_prompts_to_csv(train_dwb_successes, ATTACK_TRAIN_DWB_OUTPUT_FILE)\n",
        "\n",
        "\n",
        "    # --- 7. Run Attack on VALIDATION data (50/50 split) ---\n",
        "    val_tf_successes = run_attack(\n",
        "        val_tf_prompts, # <-- Only first 50%\n",
        "        textfooler_attack,\n",
        "        \"TextFooler (Val+Test Set)\"\n",
        "    )\n",
        "    save_prompts_to_csv(val_tf_successes, ATTACK_VAL_TF_OUTPUT_FILE)\n",
        "\n",
        "    val_dwb_successes = run_attack(\n",
        "        val_dwb_prompts, # <-- Only second 50%\n",
        "        deepwordbug_attack,\n",
        "        \"DeepWordBug (Val+Test Set)\" #<-- Fixed typo here\n",
        "    )\n",
        "    save_prompts_to_csv(val_dwb_successes, ATTACK_VAL_DWB_OUTPUT_FILE)\n",
        "\n",
        "    print(\"\\n--- Adversarial dataset generation complete. ---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using random seed: 42\n",
            "Starting adversarial attack script on TRAINING and VALIDATION data...\n",
            "Downloading NLTK resources (for textattack)...\n",
            "NLTK resources downloaded.\n",
            "\n",
            "Running on GPU: NVIDIA A100-SXM4-40GB\n",
            "\n",
            "Loading fine-tuned model from /content/drive/MyDrive/266-final-project-data/guardrail_model_DistilBERT...\n",
            "Wrapping model for TextAttack...\n",
            "Loading data from /content/drive/MyDrive/266-final-project-data/train_dataset.csv...\n",
            "Loading data from /content/drive/MyDrive/266-final-project-data/val_dataset.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Unknown if model of class <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
            "textattack: Unknown if model of class <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from /content/drive/MyDrive/266-final-project-data/test_dataset.csv...\n",
            "Loaded 12000 malicious prompts from the training set.\n",
            "Loaded 4000 malicious prompts from the combined val/test sets.\n",
            "Shuffling and splitting prompt lists for 50/50 attacks...\n",
            "\n",
            "Initializing TextFooler attack...\n",
            "\n",
            "Initializing DeepWordBug attack...\n",
            "\n",
            "--- Attacking 6000 prompts with TextFooler (Train Set) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Attacking with TextFooler (Train Set): 100%|██████████| 6000/6000 [2:39:29<00:00,  1.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TextFooler (Train Set) successfully fooled the model on 5626 out of 6000 prompts.\n",
            "\n",
            "Saving 5626 successful attack prompts to /content/drive/MyDrive/266-final-project-data/training_set_attack_textfooler.csv...\n",
            "Successfully saved file: /content/drive/MyDrive/266-final-project-data/training_set_attack_textfooler.csv\n",
            "\n",
            "--- Attacking 6000 prompts with DeepWordBug (Train Set) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Attacking with DeepWordBug (Train Set): 100%|██████████| 6000/6000 [47:39<00:00,  2.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DeepWordBug (Train Set) successfully fooled the model on 5134 out of 6000 prompts.\n",
            "\n",
            "Saving 5134 successful attack prompts to /content/drive/MyDrive/266-final-project-data/training_set_attack_deepwordbug.csv...\n",
            "Successfully saved file: /content/drive/MyDrive/266-final-project-data/training_set_attack_deepwordbug.csv\n",
            "\n",
            "--- Attacking 2000 prompts with TextFooler (Val+Test Set) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Attacking with TextFooler (Val+Test Set): 100%|██████████| 2000/2000 [50:50<00:00,  1.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TextFooler (Val+Test Set) successfully fooled the model on 1872 out of 2000 prompts.\n",
            "\n",
            "Saving 1872 successful attack prompts to /content/drive/MyDrive/266-final-project-data/validation_set_attack_textfooler.csv...\n",
            "Successfully saved file: /content/drive/MyDrive/266-final-project-data/validation_set_attack_textfooler.csv\n",
            "\n",
            "--- Attacking 2000 prompts with DeepWordBug (Val+Test Set) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Attacking with DeepWordBug (Val+Test Set): 100%|██████████| 2000/2000 [15:36<00:00,  2.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DeepWordBug (Val+Test Set) successfully fooled the model on 1732 out of 2000 prompts.\n",
            "\n",
            "Saving 1732 successful attack prompts to /content/drive/MyDrive/266-final-project-data/validation_set_attack_deepwordbug.csv...\n",
            "Successfully saved file: /content/drive/MyDrive/266-final-project-data/validation_set_attack_deepwordbug.csv\n",
            "\n",
            "--- Adversarial dataset generation complete. ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv9ySXkxvkLK",
        "outputId": "c67d87b7-1802-41a3-bb83-d0c17072c81a"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
